# Abstract:
We are interested in how neural networks can automatically describe the content of images using natural language. In order to gain a deeper insight into this computer vision problem statement, we have decided to implement the current state-of-the-art image caption generator. Our neural network based image caption generator is implemented in Python powered by the Tensorflow library. We have identified four major modules in our experiment: 

1. Data pre-processing;
2. Convolutional Neural Network (CNN)
3. Transformer with single self-attention and cross attention
4. Transformer with two self-attention and cross attention with residual connection


 # 1. RNN Model

A Recurrent Neural Network is basically an enhanced version of the traditional neural nets, with a key element of the hidden state, which will act as an intermediate step for the next nodes in the network. The information is processed sequentially and it has different versions too, vanilla RNN, LSTM, and GRU units.c(we use GRUs as they are computationally more efficient than LSTMs and give comparable accuracy).


## CNN Model: 

We first use pre-trained CNN (Efficient net) to get image encodings in the form of vectors that are then fed into the GRU decoder as hidden states. In this part of the architecture, we will use the pre-trained “Efficient Net B4” network. Here, we will use the imagenet weights with top is equal to false, allowing us to exclude the top layer of the network.
![CNN](/assets/CNN.png)

## Decoder (RNN): 
In this part of the Model we use RNN (GRU) for generating captions as shown in the diagram. The RNN takes in the starting sequence of the caption (in our case) and the next word is predicted which is appended back with the input caption sequence and fed into the RNN till the end sequence. The RNN layers take in Image vector output from the CNN blocks as hidden states so that the RNN can use the context of the image and also the previous decoder sequence to generate the next word in the sequence.

### __Captions Generated by RNN model:__
![Picture1](/assets/RNN_Captions.png)
From the Captions above the RNN model performed well on the dog images like on the bottom right image, RNN has generated fair captions but on the top two images, the captions are not relevant. For the 1st image, the context of football is learned from the field but the woman is interpreted as a man. For the 2nd image, the water is being interpreted as a fountain. Similarly, in the 3rd image, the context of a man and the water is learned properly but the action of fishing is wrongly interpreted.

### Accuracy and loss plot for RNN model:
![RNN_Accuracy](/assets/RNN_Accuracy.png)
We see from the plot, the RNN model took 20 epochs to train and converge before the early callback. The maximum BLEU accuracy achieved by the RNN model on the Validation data is around 39%.

# 2. Transformer Model (with single self-attention and cross attention) 

## CNN Model: 
In this part of the architecture, we will use the pre-trained “Efficient Net B4” network. Here, we will use the imagenet weights with top is equal to false, allowing us to exclude the top layer of the network.

## Embeddings(GloVe): 
We can either use pre-trained embeddings like GloVe, word2vec, etc or can randomly initialize the embedding matrix.
 
## Positional Embeddings: 
Same word might have a different meaning in two sentences with its position. For example, John’s horse is cute and John looks like a horse with uncombed hair. So, we need a mechanism that will consider context based on the position of a word in a sentence, therefore we will positional embedding vectors. In our problem statement, we will use vectors of length [0 to length(caption)] to get the positional embeddings. Once we have the positional embeddings, then we will combine them with the standard embeddings before passing them on the encoder block. 
 
## Encoder Block: 
Now, the output of the positional embeddings along with the input image data will be passed as input into the encoder block. It will go through the multi-headed attention layer and a feed-forward/dense layer.
The attention layer will emphasize what part of the network we should focus on, to learn the abstractions, such as how relevant a word is in a sentence, with relevant to other words in the same sentence. The feed-forward/dense layer is used to transform the output of the attention layer into a format that can be easily used by the next encoder or the decoder block.
 
## Decoder Block: 
The decoder block has a similar structure as the encoder block and all the magic happens in the decoder block of the transformers. The processing of vectors in the decoder is very similar to the encoder, the only difference is that the decoder also computes the attention between the decoder vectors and the output from the encoder block. The primary reason for doing this is to focus only on the relevant part of the input vectors. Subsequently, the output of the decoder block is passed to the softmax layers, which will spit out the probability of each word, and based on the maximum probability we will transform back to the original words from the vocabulary using the argmax function internally. 

## Masking: 
We can use a mask to randomly mask some words from the sequence and then try to predict the word with the max prob output from the softmax layer. This will help to learn the abstraction in a much better and generalized way.

### Vanilla Transformer (6 heads)
![Transformer_1](/assets/Transformer_1.png)


### Captions Generated by Transformer model:
![Transformer_1_Captions](/assets/Transformer_1_Captions.png)

From the Captions above, the Transformer model performed better than the RNN model in understanding the subjects but the RNN model was better in understanding the context of the images for the most part. 
For the 1st image, the woman was identified as a little girl but her gender was interpreted correctly unlike RNN. For the 2nd image, the woman smiling is interpreted better. Similarly, in the 3rd image, the context of a man standing on something is interpreted properly. The 4th image shows the dog interpreted properly but the background is not interpreted properly.
These shortcomings can be attributed to the fact that the Transformer model had 6 heads in self-attention and cross attention, so because of improper gradient flow there is no proper tuning of the weights, this has led to less feature extraction. The model was successfully able to recognize the object but the generalizing on the background pixels of the image and the colors were not good enough because the weights were not able to learn the background information.


### Accuracy and loss plot for Transformer  model:
![Transformer1_Acc](/assets/Transformer1_Acc.png)
 
From the above accuracy plot we can see that the maximum BLEU accuracy attained by the model on Validation data is 37% which is lesser than RNN. 
We see that the accuracy on the training data goes on increasing this suggests the overfitting, but this overfitting is in the linear or feed-forward layers, i.e. after feature extraction from the attention layers the feed-forward layers are getting overfitted on the training data, although the self-attention heads are not tuned properly extracting the context.



# 3. Transformer Model (with two self-attention and cross attention with residual connection) 

### Residual Transformer (3-3 heads)
![Transformer2](/assets/Transformer2.png)

Keeping the fundamentals of CNN, Encoder-Decoder of a Transformer, and understanding the shortcomings of the previous transformer model, we tried to come up with a better architecture, 
●	Which have 2 self-attention and cross attention heads of 3 instead of a single attention layer of 6 heads.
●	The output tensors of the attention layers are added with input tensors again before feeding into the next attention layer.
●	Also, the (self and cross) attention part of the decoder provided a direct residual connection to the inputs.

Thus we tried to tackle the underperformance of the model by providing a better gradient flow. And also since there is a direct connection with the decoding sequence the positional context of the input caption also is better propagated.

### Captions Generated by Transformer model:

![Transformer2_Captions](/assets/Transformer2_Captions.png)

From the Captions above, the residual Transformer model performed better than both the models as we can see the captions generated on all 4 test images were relevant and the model was able to learn the context of the background of the images too.

For the 1st image, the woman was identified correctly and the sports field in the background was also interpreted. For the 2nd image, the woman smiling and her dress colour is interpreted correctly. In the 3rd image, the caption of a man standing on the rock is properly put together and the water behind him is interpreted as a lake. The 4th image is a perfect caption

### Accuracy and loss plot for model :

![Transformer2_Acc](/assets/Transformer2_Acc.png)

From the above plots, we see the model reached the maximum BLEU score of 40% on training data. Also, we see that the training and validation accuracies are closer than the previous transformer plot denoting better convergence than before.


# Limitations
The limitations we encountered during the projects were
1.	The transformer model (2nd) although worked better is computationally intensive and requires a lot of train time, on the other hand, the RNN network is much quicker and the captions are fairly good. We have limited both the models to the total number of 6 heads in self and cross attention. 

2.	The captions generated in all the models were good only for the images which the model had previously encountered. The models were able to properly recognize the object but the background context was not learned until there were more images with similar background context.
In the 1st image, the caption model fails as it interprets the parachute as a camp which is logically wrong but the pixelated information might be analogous to camping images on which the model was trained.
In the second image, the caption is perfect till swimming but it fails to recognize the waterfall in the background. This was because there were a lot of swimming pool images but the waterfall images were low.

![Limitations](/assets/Limitations.png) 	 

3.	The main limitation was the limited corpus of caption and image data, the transformer model is data-hungry thus a larger image and caption decision is required to generate more meaningful captions.


## Background/Related Work: 
This section discusses relevant literature for this project
https://fuqichen1998.github.io/pdfs/eecs442_report.pdf
https://github.com/Dantekk/Image-Captioning/
