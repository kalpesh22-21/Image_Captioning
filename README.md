# Abstract:
We are interested in how neural networks can automatically describe the content of images using natural language. In order to gain a deeper insight into this computer vision problem statement, we have decided to implement the current state-of-the-art image caption generator. Our neural network based image caption generator is implemented in Python powered by the Tensorflow library. We have identified four major modules in our experiment: 

1. Data pre-processing;
2. Convolutional Neural Network (CNN)
3. Transformer with single self-attention and cross attention
4. Transformer with two self-attention and cross attention with residual connection


 # 1. RNN Model

A Recurrent Neural Network is basically an enhanced version of the traditional neural nets, with a key element of the hidden state, which will act as an intermediate step for the next nodes in the network. The information will flow through chains like structures before passing it down to the cells in future timesteps. These cells have a memory unit to retain information and act as a link between past information and the present. The information is processed sequentially and it has different versions too, vanilla RNN, LSTM, and GRU units.

This is an example of many to many use cases for RNNs (we use GRUs as they are computationally more efficient than LSTMs and give comparable accuracy).


## CNN Model: 

We first use pre-trained CNN (Efficient net) to get image encodings in the form of vectors that are then fed into the GRU decoder as hidden states. In this part of the architecture, we will use the pre-trained “Efficient Net B4” network. Here, we will use the imagenet weights with top is equal to false, allowing us to exclude the top layer of the network.


## Decoder (RNN): 
In this part of the Model we use RNN (GRU) for generating captions as shown in the diagram. The RNN takes in the starting sequence of the caption (in our case) and the next word is predicted which is appended back with the input caption sequence and fed into the RNN till the end sequence. The RNN layers take in Image vector output from the CNN blocks as hidden states so that the RNN can use the context of the image and also the previous decoder sequence to generate the next word in the sequence.

### Captions Generated by RNN model:


From the Captions above the RNN model performed well on the dog images like on the bottom right image, RNN has generated fair captions but on the top two images, the captions are not relevant. For the 1st image, the context of football is learned from the field but the woman is interpreted as a man. For the 2nd image, the water is being interpreted as a fountain. Similarly, in the 3rd image, the context of a man and the water is learned properly but the action of fishing is wrongly interpreted.

### Accuracy and loss plot for RNN model:

We see from the plot, the RNN model took 20 epochs to train and converge before the early callback. The maximum BLEU accuracy achieved by the RNN model on the Validation data is around 39%.
        




## Background/Related Work: 
This section discusses relevant literature for this project
https://fuqichen1998.github.io/pdfs/eecs442_report.pdf
Danteantekk/Image-Captioning: Image Captioning using CNN and Transformer.(github.com)
